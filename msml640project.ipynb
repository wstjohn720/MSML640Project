{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pillow pandas scikit-learn plotly"
      ],
      "metadata": {
        "id": "26M-QsN46JBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library"
      ],
      "metadata": {
        "id": "0bUIy2j_jZVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wezNk-U2gjln"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INpe7b9spvL-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Unzip the dataset here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "!unzip /content/drive/MyDrive/50States10K.zip -d /content/data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "630UrEEIjb1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m1540JRzRguj"
      },
      "outputs": [],
      "source": [
        "def geo_loss(outputs,label,long,lat):\n",
        "  device = outputs.device\n",
        "\n",
        "  long = long.to(device)\n",
        "  lat = lat.to(device)\n",
        "\n",
        "  long1 = torch.deg2rad(long[outputs])\n",
        "  long2 = torch.deg2rad(long[label])\n",
        "  lat1 = torch.deg2rad(lat[outputs])\n",
        "  lat2 = torch.deg2rad(lat[label])\n",
        "\n",
        "  a = torch.sin((lat1-lat2)/2)**2 + torch.cos(lat1)*torch.cos(lat2)*torch.sin((long1-long2)/2)**2\n",
        "\n",
        "  return torch.atan2(torch.sqrt(a),1-torch.sqrt(a))\n",
        "\n",
        "def find_k_nearest_states(state_longs, state_lats,k=3):\n",
        "    N = len(state_lats)\n",
        "    nearest = []\n",
        "\n",
        "    for i in range(N):\n",
        "\n",
        "        others = torch.tensor([j for j in range(N)])\n",
        "\n",
        "        dists = geo_loss(\n",
        "            torch.full((len(others),), i),\n",
        "            others,\n",
        "            state_longs,\n",
        "            state_lats\n",
        "        )\n",
        "        topk = torch.topk(dists, k+1, largest=False).indices\n",
        "        nearest.append(others[topk])\n",
        "\n",
        "\n",
        "    return nearest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cm66VJ2W1oBO"
      },
      "outputs": [],
      "source": [
        "class AveragedGroupDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, transform=None,num_class=50):\n",
        "        self.base_dataset = ImageFolder(root=root)\n",
        "        self.angle = [\"0\",\"90\",\"180\",\"270\"]\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "        self.groups = dict()\n",
        "        for idx, (path, label) in enumerate(self.base_dataset.samples):\n",
        "\n",
        "            prefix = path.rsplit('_',1)[0]\n",
        "            if prefix:\n",
        "                key = (label, prefix)\n",
        "                if key not in self.groups:\n",
        "                  flag = True\n",
        "                  for theta in self.angle:\n",
        "                    filename = f\"{prefix}_{theta}.jpg\"\n",
        "                    if not os.path.exists(filename):\n",
        "                      flag = False\n",
        "                      break\n",
        "                  if flag:\n",
        "                    self.groups[key] = idx\n",
        "        self.groups_key = list(self.groups.keys())\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.groups)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, prefix = self.groups_key[idx]\n",
        "\n",
        "        images = []\n",
        "        for theta in self.angle:\n",
        "            img_path = f\"{prefix}_{theta}.jpg\"\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            images.append(img)\n",
        "\n",
        "        return images, label\n",
        "\n",
        "\n",
        "class SplitedGroupDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, groups_key,transform=None):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.angle = [\"0\",\"90\",\"180\",\"270\"]\n",
        "        self.transform = transform\n",
        "        self.groups_key = groups_key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.groups_key)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, prefix = self.groups_key[idx]\n",
        "\n",
        "        images = []\n",
        "        for theta in self.angle:\n",
        "            img_path = f\"{prefix}_{theta}.jpg\"\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            images.append(img)\n",
        "\n",
        "        return images, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dW_30TLr5FP4"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = AveragedGroupDataset('/content/data', transform=transform)\n",
        "split = dataset.groups_key\n",
        "random.shuffle(split)\n",
        "#this n control the size of the dataset we used for the model.\n",
        "n = len(dataset)//5\n",
        "split = split[:n]\n",
        "#this n control the proportion of test and training set\n",
        "n = 4*n//5\n",
        "train = split[:n]\n",
        "test = split[n:]\n",
        "trainset = SplitedGroupDataset(dataset.base_dataset,train,transform)\n",
        "testset = SplitedGroupDataset(dataset.base_dataset,test,transform)\n",
        "dataloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True,num_workers=4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True,num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model setup and training\n"
      ],
      "metadata": {
        "id": "UT2Ptuo4jnlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MedIntResNet(nn.Module):\n",
        "    def __init__(self, num_classes=50, num_views=4):\n",
        "        super(MedIntResNet, self).__init__()\n",
        "        base_resnet = resnet18(pretrained=True)\n",
        "\n",
        "\n",
        "        self.shared_conv = nn.Sequential(*list(base_resnet.children())[:-2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "\n",
        "        self.feature_dim = 512\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(self.feature_dim * num_views, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feats = []\n",
        "        for view in inputs:\n",
        "            x = self.shared_conv(view)\n",
        "            x = self.avgpool(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            feats.append(x)\n",
        "\n",
        "        combined = torch.cat(feats, dim=1)\n",
        "        out = self.fc(combined)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gCF1SMi-4VUL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JENrMxkGJE-z"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "long = [\n",
        "    -86.7911, -152.4044, -111.4312, -92.3731, -119.6816, -105.3111, -72.7554, -75.5071, -81.6868, -83.6431,\n",
        "    -157.4983, -114.4788, -88.9861, -86.2583, -93.2105, -96.7265, -84.6701, -91.8678, -69.3819, -76.8021,\n",
        "    -71.5301, -84.5361, -93.9002, -89.6787, -92.2884, -110.4544, -98.2681, -117.0554, -71.5639, -74.521,\n",
        "    -106.2485, -74.9481, -79.8064, -99.784, -82.7649, -96.9289, -122.0709, -77.2098, -71.5118, -80.945,\n",
        "    -99.4388, -86.6923, -97.5635, -111.8624, -72.7107, -78.17, -121.4905, -80.9545, -89.6165, -107.3025\n",
        "]\n",
        "long = torch.tensor(long)\n",
        "long.to(device)\n",
        "\n",
        "lat = [\n",
        "    32.8067, 61.3707, 33.7298, 34.9697, 36.1162, 39.0598, 41.5978, 39.3185, 27.7663, 33.0406,\n",
        "    21.0943, 44.2405, 40.3495, 39.8494, 42.0115, 38.5266, 37.6681, 31.1695, 44.6939, 39.0639,\n",
        "    42.2302, 43.3266, 45.6945, 32.7416, 38.4561, 46.9219, 41.1254, 38.3135, 43.4525, 40.2989,\n",
        "    34.8405, 42.1657, 35.6301, 47.5289, 40.3888, 35.5653, 44.572, 40.5908, 41.6809, 33.8569,\n",
        "    44.2998, 35.7478, 31.0545, 40.15, 44.0459, 37.7693, 47.4009, 38.4912, 44.2685, 42.7559\n",
        "]\n",
        "\n",
        "lat = torch.tensor(lat)\n",
        "lat.to(device)\n",
        "k = 3\n",
        "nearest = find_k_nearest_states(lat, long, k=3)\n",
        "nearest = torch.stack(nearest).to(device)\n",
        "\n",
        "\n",
        "num_classes = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "6ukqzwpYjrwu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTOyR7az6Ak7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = MedIntResNet(num_classes=num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 50\n",
        "w = 0.5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = [v.to(device) for v in inputs]\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        pred_class = outputs.argmax(dim=1)\n",
        "        loss = (1-w)*F.cross_entropy(outputs, labels) + w*geo_loss(pred_class,labels,long,lat).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "all_preds= []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs = [v.to(device) for v in inputs]\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        nearest_k = nearest[labels]\n",
        "        in_top_k = (nearest_k == predicted.unsqueeze(1)).any(dim=1)\n",
        "        top_k_accuracy = in_top_k.float().mean().item()\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Top {k} Nearest States Accuracy: {top_k_accuracy * 100:.2f}%\")\n",
        "\n",
        "precision = precision_score(all_labels, all_preds, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "\n",
        "recall = recall_score(all_labels, all_preds, average='macro')\n",
        "f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F-1 score:\", f1)"
      ],
      "metadata": {
        "id": "du99YRPrccsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8596401d-cace-4b07-832b-09da84ee2562"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 42.18%\n",
            "Top 3 Nearest States Accuracy: 75.00%\n",
            "Precision: 0.4139674774969325\n",
            "Recall: 0.41770981207340463\n",
            "F-1 score: 0.41089014927622464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization and application"
      ],
      "metadata": {
        "id": "0-RNKDzkiy-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_instance_images(base_path, instance_id):\n",
        "    angle = ['0', '90', '180', '270']\n",
        "    imgs = []\n",
        "    for view in angle:\n",
        "        path = os.path.join(base_path, f'{instance_id}_{view}.jpg')\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img = transform(img)\n",
        "        imgs.append(img)\n",
        "    return imgs"
      ],
      "metadata": {
        "id": "VcDbg5yl_oFH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "us_states = [\n",
        "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA',\n",
        "    'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD',\n",
        "    'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',\n",
        "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC',\n",
        "    'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
        "]\n",
        "\n",
        "data = {\n",
        "    'state': us_states,\n",
        "\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#change the test_root to the name of the folder with the testing images\n",
        "test_root = \"/content/new_images\"\n",
        "model.eval()\n",
        "#change the id to \"IMG1\" to switch the testing image\n",
        "instance_id = 'IMG2'\n",
        "###############################################\n",
        "imgs = load_instance_images(test_root, instance_id)\n",
        "imgs = [img.unsqueeze(0).to(device) for img in imgs]\n",
        "with torch.no_grad():\n",
        "    output = model(imgs)\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    pred = us_states[probs.argmax()]\n",
        "    confidence = probs.max(dim=1).values.item()\n",
        "\n",
        "    probs = probs.squeeze().tolist()\n",
        "    df['percentage'] = [p*100 for p in probs]\n",
        "    fig = px.choropleth(\n",
        "    df,\n",
        "    locations='state',\n",
        "    locationmode=\"USA-states\",\n",
        "    color='percentage',\n",
        "    scope=\"usa\",\n",
        "    color_continuous_scale='Blues',\n",
        "    labels={'percentage': 'Percentage'}\n",
        "    )\n",
        "\n",
        "    fig.update_layout(title_text=f'These images might come from {pred} with confidence: {confidence:.2f}')\n",
        "    fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "fi0_Mg_CxTXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "09114211-6c67-4adc-ba8f-94fba9e417f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"aa3d5577-e2a7-48b7-9ee1-bffdb1ae1c0e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"aa3d5577-e2a7-48b7-9ee1-bffdb1ae1c0e\")) {                    Plotly.newPlot(                        \"aa3d5577-e2a7-48b7-9ee1-bffdb1ae1c0e\",                        [{\"coloraxis\":\"coloraxis\",\"geo\":\"geo\",\"hovertemplate\":\"state=%{location}\\u003cbr\\u003ePercentage=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"locationmode\":\"USA-states\",\"locations\":[\"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\"],\"name\":\"\",\"z\":[0.00018838720734493108,0.0005830142072227318,4.442981094143761e-7,0.00032012571864470374,0.0081652331573423,0.0019300086933071725,6.5210237210067135e-6,0.6386062130331993,0.0009146045158558991,54.16530966758728,1.238664060210226e-7,0.000032943995620371425,0.0017774309526430443,5.4896606371812595e-6,6.817280073789789e-7,4.904623551738041e-6,0.00025310289402113995,1.916846592564525e-6,0.0018814151189872064,2.803989127278328,0.01299097784794867,0.000015216937754303217,5.1665100642139805e-6,0.00035091297831968404,0.00620617502136156,0.000951773199631134,0.003026968079211656,0.000018896471942753124,0.00007212682362478517,0.000035862856861967884,2.1916431336421738e-6,0.0006123673756519565,40.528592467308044,3.033018725062675e-7,0.008085235458565876,1.4687107530164667e-6,0.00006908173304509546,0.0022065996745368466,0.000469694805360632,1.5847237780690193,7.748908181781644e-8,0.000663005403112038,0.004321956657804549,0.00017664157212493592,0.11572871590033174,0.10482518700882792,0.00016803496691863984,0.0010884538824029732,0.0006155238679639297,1.2336960342196335e-7],\"type\":\"choropleth\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"geo\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"center\":{},\"scope\":\"usa\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Percentage\"}},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"These images might come from GA with confidence: 0.54\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('aa3d5577-e2a7-48b7-9ee1-bffdb1ae1c0e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model loding\n"
      ],
      "metadata": {
        "id": "j0LTIIuMj2A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#only to save the model, not need for testing\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n"
      ],
      "metadata": {
        "id": "n3n-s8UAhHgs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run this if want to load the model\n",
        "model = MedIntResNet(num_classes=num_classes)\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "XJgPKkBQivyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c2aa5e-09c0-4aa7-897a-30a6e7105d91"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning:\n",
            "\n",
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning:\n",
            "\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MedIntResNet(\n",
              "  (shared_conv): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}